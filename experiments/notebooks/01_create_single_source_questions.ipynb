{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff50b44c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up directory paths\n",
        "notebook_path = Path().resolve()  # Path to the current notebook directory\n",
        "experiments_path = notebook_path.parent # Path to the experiments directory where the utils module is located\n",
        "backend_path = experiments_path.parent / \"backend\"\n",
        "\n",
        "if experiments_path not in sys.path:\n",
        "    sys.path.insert(0, str(experiments_path))\n",
        "\n",
        "# Load environment variables from backend/.env if it exists;\n",
        "# otherwise, load system environment variables with default behavior\n",
        "env_path = backend_path / \".env\"\n",
        "if env_path.exists():\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"[OK] Loaded environment variables from {env_path}\")\n",
        "else:\n",
        "    load_dotenv(override=True)\n",
        "    print(\"[WARNING] Backend .env not found, using default environment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2143d21f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Local Utilities ---\n",
        "from utils.handbook_loader import load_handbook_documents\n",
        "\n",
        "# Load all handbook documents from the backend data/handbook directory\n",
        "all_documents = load_handbook_documents(backend_path / \"data\" / \"handbook\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b5a12a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "from typing import List\n",
        "from utils.prompts import QA_GENERATION_SYSTEM_PROMPT\n",
        "from utils.models import QAPairList, HandbookDoc, HandbookDocMetadata, QAPairWithTS\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core QA generation utility function\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def generate_qa_pairs_from_single_document(\n",
        "    document: HandbookDoc,\n",
        "    num_questions: int = 3,\n",
        "    model: str = 'groq/openai/gpt-oss-20b'\n",
        ") -> List[QAPairWithTS]:\n",
        "    \"\"\"\n",
        "    Generate search-style factoid QA pairs from a single document string.\n",
        "\n",
        "    Args:\n",
        "        document: The document object.\n",
        "        num_questions: Number of QA pairs to generate.\n",
        "        model: Model name to use for LLM.\n",
        "\n",
        "    Returns:\n",
        "        List of QAPairWithTS objects representing question-answer pairs enriched with source document metadata.\n",
        "    \"\"\"\n",
        "    user_prompt = (\n",
        "        f\"The user has provided the following document:\\n\\n\"\n",
        "        f\"[DOCUMENT BEGINS]\\n\\n{document.content}\\n\\n[DOCUMENT ENDS]\\n\\n\\n\"\n",
        "        f\"Generate {num_questions} question-answer pairs from the document.\\n\\n\"\n",
        "        \"Reply only with the question-answer pairs, nothing else.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": QA_GENERATION_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    response = completion(model=model, messages=messages, response_format=QAPairList)\n",
        "    reply = response.choices[0].message.content\n",
        "\n",
        "    return [\n",
        "        QAPairWithTS(**pair.model_dump(), question_type='single-source', doc_metadata=[HandbookDocMetadata(**document.model_dump(exclude={'content'}))])\n",
        "        for pair in QAPairList.model_validate_json(reply).pairs\n",
        "    ]\n",
        "\n",
        "qa_pairs = generate_qa_pairs_from_single_document(all_documents[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6a7ecb",
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f40dd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.prompts import GROUNDEDNESS_CRITIQUE_SYSTEM_PROMPT, RELEVANCE_CRITIQUE_SYSTEM_PROMPT, STANDALONE_CRITIQUE_SYSTEM_PROMPT\n",
        "from utils.models import QuestionCritique, QuestionCritiqueWithType, CritiqueType\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Unified critique model and prompt mapping\n",
        "# ---------------------------------------------------------------------------\n",
        " \n",
        "CRITIQUE_PROMPTS: dict[CritiqueType, str] = {\n",
        "    \"groundedness\": GROUNDEDNESS_CRITIQUE_SYSTEM_PROMPT,\n",
        "    \"relevance\": RELEVANCE_CRITIQUE_SYSTEM_PROMPT,\n",
        "    \"standalone\": STANDALONE_CRITIQUE_SYSTEM_PROMPT,\n",
        "}\n",
        "\n",
        "def critique_question(\n",
        "    context: str,\n",
        "    question: str,\n",
        "    critique_type: CritiqueType,\n",
        "    model: str = 'groq/openai/gpt-oss-20b',\n",
        ") -> QuestionCritiqueWithType:\n",
        "    \"\"\"\n",
        "    Critique a question against a context using a specified evaluation dimension.\n",
        "\n",
        "    Args:\n",
        "        context: The source document text.\n",
        "        question: The question to evaluate.\n",
        "        critique_type: One of \"groundedness\", \"relevance\", or \"standalone\".\n",
        "        model: Model name to use for the LLM call.\n",
        "\n",
        "    Returns:\n",
        "        A QuestionCritiqueWithType with a rationale, score, and critique type.\n",
        "    \"\"\"\n",
        "    system_prompt = CRITIQUE_PROMPTS[critique_type]\n",
        "    user_prompt = (\n",
        "        f\"The user has provided the following question:\\n\\n\"\n",
        "        f\"[QUESTION BEGINS]\\n\\n{question}\\n\\n[QUESTION ENDS]\\n\\n\"\n",
        "        f\"The user has provided the following context:\\n\\n\"\n",
        "        f\"[CONTEXT BEGINS]\\n\\n{context}\\n\\n[CONTEXT ENDS]\\n\\n\\n\"\n",
        "        \"Reply only with your rationale for the rating and your score (1-5), nothing else.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    response = completion(model=model, messages=messages, response_format=QuestionCritique)\n",
        "    reply = response.choices[0].message.content\n",
        "    return QuestionCritiqueWithType(\n",
        "        **QuestionCritique.model_validate_json(reply).model_dump(),\n",
        "        critique_type=critique_type,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ea637e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def critique_all_dimensions(context: str, question: str) -> List[QuestionCritiqueWithType]:\n",
        "    \"\"\"\n",
        "    Critique a question against a context across all supported evaluation dimensions.\n",
        "\n",
        "    Args:\n",
        "        context: The source document text.\n",
        "        question: The question to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        A list of QuestionCritiqueWithType instances, one per critique dimension.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        critique_question(context, question, crit_type)\n",
        "        for crit_type in [\"relevance\", \"standalone\", \"groundedness\"]\n",
        "    ]\n",
        "\n",
        "# Test the unified critique function across all three dimensions\n",
        "context = all_documents[0].content\n",
        "question = qa_pairs[0].question\n",
        "critiques = critique_all_dimensions(context, question)\n",
        "\n",
        "print(question)\n",
        "for critique in critiques:\n",
        "    print(f\"{critique.critique_type}: {critique.rationale} (score: {critique.score})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc996c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "critiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d8e989",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.models import QAPairEvalRecord\n",
        "\n",
        "doc_content_lookup = {doc.id: doc.content for doc in all_documents}\n",
        "\n",
        "eval_records = [\n",
        "    QAPairEvalRecord(\n",
        "        **qa_pair.model_dump(),\n",
        "        critiques=critique_all_dimensions(\"\\n\\n---\\n\\n\".join(doc_content_lookup[m.id] for m in qa_pair.doc_metadata), qa_pair.question),\n",
        "    )\n",
        "    for qa_pair in qa_pairs\n",
        "]\n",
        "\n",
        "eval_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21100a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_eval_dataset(\n",
        "    documents: List[HandbookDoc],\n",
        "    n_docs: int = 10,\n",
        "    seed: int = 42,\n",
        ") -> List[QAPairEvalRecord]:\n",
        "    \"\"\"\n",
        "    Sample n_docs documents and run the full QA generation + critique pipeline.\n",
        "\n",
        "    Args:\n",
        "        documents: Full list of handbook documents to sample from.\n",
        "        n_docs: Number of documents to sample (default 10).\n",
        "        seed: Random seed for reproducible document sampling (default 42).\n",
        "\n",
        "    Returns:\n",
        "        List of QAPairEvalRecord instances with critiques across all dimensions.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    sampled_docs = random.sample(documents, min(n_docs, len(documents)))\n",
        "    doc_content_lookup = {doc.id: doc.content for doc in sampled_docs}\n",
        "\n",
        "    all_qa_pairs: List[QAPairWithTS] = []\n",
        "    for doc in sampled_docs:\n",
        "        all_qa_pairs.extend(generate_qa_pairs_from_single_document(doc))\n",
        "\n",
        "    return [\n",
        "        QAPairEvalRecord(\n",
        "            **qa_pair.model_dump(),\n",
        "            critiques=critique_all_dimensions(\n",
        "                \"\\n\\n---\\n\\n\".join(doc_content_lookup[m.id] for m in qa_pair.doc_metadata), qa_pair.question\n",
        "            ),\n",
        "        )\n",
        "        for qa_pair in all_qa_pairs\n",
        "    ]\n",
        "\n",
        "eval_records = generate_eval_dataset(all_documents, n_docs=5, seed=42)\n",
        "print(f\"Generated {len(eval_records)} eval records from 5 sampled documents.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6646255a",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path(\".\")  # same directory as the notebook\n",
        "\n",
        "def save_eval_jsonl(records: List[QAPairEvalRecord], path: Path) -> None:\n",
        "    \"\"\"Serialize all eval records to a JSONL file.\"\"\"\n",
        "    path = Path(path)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for record in records:\n",
        "            f.write(record.model_dump_json() + \"\\n\")\n",
        "    print(f\"Saved {len(records)} records → {path}\")\n",
        "\n",
        "\n",
        "def save_filtered_eval_jsonl(\n",
        "    records: List[QAPairEvalRecord],\n",
        "    path: Path,\n",
        "    min_score: int = 4,\n",
        ") -> List[QAPairEvalRecord]:\n",
        "    \"\"\"\n",
        "    Keep only records where every critique dimension scores >= min_score, then save to JSONL.\n",
        "\n",
        "    Returns the filtered list for downstream use.\n",
        "    \"\"\"\n",
        "    filtered = [\n",
        "        r for r in records\n",
        "        if all(c.score >= min_score for c in r.critiques)\n",
        "    ]\n",
        "    save_eval_jsonl(filtered, path)\n",
        "    print(\n",
        "        f\"Filtered to {len(filtered)} / {len(records)} records \"\n",
        "        f\"(all dimensions >= {min_score})\"\n",
        "    )\n",
        "    return filtered\n",
        "\n",
        "\n",
        "save_eval_jsonl(eval_records, OUTPUT_DIR / \"eval_questions.jsonl\")\n",
        "filtered_records = save_filtered_eval_jsonl(\n",
        "    eval_records, OUTPUT_DIR / \"eval_questions_filtered.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4fc678",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "SCORE_DIMS = [\"relevance\", \"standalone\", \"groundedness\"]\n",
        "\n",
        "\n",
        "def build_eval_dataframe(records: List[QAPairEvalRecord]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a tidy DataFrame from eval records — one row per question.\n",
        "\n",
        "    Columns: id, question, answer, question_type, doc_title, doc_category,\n",
        "             relevance, standalone, groundedness.\n",
        "\n",
        "    The integer 'id' (0-indexed) is the shared key that links rows here\n",
        "    to entries in the markdown export and JSONL files.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i, record in enumerate(records):\n",
        "        score_map = {c.critique_type: c.score for c in record.critiques}\n",
        "        rows.append({\n",
        "            \"id\": i,\n",
        "            \"question\": record.question,\n",
        "            \"answer\": record.answer,\n",
        "            \"question_type\": record.question_type,\n",
        "            \"doc_title\": \" | \".join(m.title for m in record.doc_metadata),\n",
        "            \"doc_category\": \" | \".join(m.category for m in record.doc_metadata),\n",
        "            \"relevance\": score_map.get(\"relevance\"),\n",
        "            \"standalone\": score_map.get(\"standalone\"),\n",
        "            \"groundedness\": score_map.get(\"groundedness\"),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "df = build_eval_dataframe(eval_records)\n",
        "df.to_csv(OUTPUT_DIR / \"eval_questions.csv\", index=False)\n",
        "print(f\"Saved DataFrame ({df.shape[0]} rows × {df.shape[1]} cols) → eval_questions.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01229c64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_eval_record_md(record: QAPairEvalRecord, id: int) -> str:\n",
        "    \"\"\"\n",
        "    Render a single eval record as a Markdown block.\n",
        "\n",
        "    Uses native Markdown (headings, tables, bold) so the file renders cleanly\n",
        "    in any Markdown viewer. The 'ID: <n>' heading is Ctrl+F friendly and maps\n",
        "    directly to the 'id' column in the DataFrame.\n",
        "    \"\"\"\n",
        "    SCORE_EMOJI = {1: \"★☆☆☆☆\", 2: \"★★☆☆☆\", 3: \"★★★☆☆\", 4: \"★★★★☆\", 5: \"★★★★★\"}\n",
        "\n",
        "    lines = []\n",
        "\n",
        "    # Header — easy to Ctrl+F\n",
        "    lines.append(f\"## ID: {id}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Q / A / metadata\n",
        "    lines.append(f\"**Q:** {record.question}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"**A:** {record.answer}\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\n",
        "        f\"**Type:** {record.question_type} &nbsp;|&nbsp; \"\n",
        "        f\"**Source:** {\", \".join(f\"{m.title} [{m.category}]\" for m in record.doc_metadata)}\"\n",
        "    )\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Score summary table\n",
        "    lines.append(\"| Dimension | Score | Rating |\")\n",
        "    lines.append(\"|-----------|:-----:|--------|\")\n",
        "    for c in record.critiques:\n",
        "        lines.append(\n",
        "            f\"| {c.critique_type.capitalize()} \"\n",
        "            f\"| {c.score} / 5 \"\n",
        "            f\"| {SCORE_EMOJI.get(c.score, '')} |\"\n",
        "        )\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Per-dimension rationale\n",
        "    for c in record.critiques:\n",
        "        lines.append(f\"**{c.critique_type.capitalize()}** — {c.score}/5\")\n",
        "        lines.append(\"\")\n",
        "        lines.append(f\"> {c.rationale}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "    lines.append(\"---\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def export_eval_markdown(records: List[QAPairEvalRecord], path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Export eval records to a Markdown file with proper Markdown formatting.\n",
        "\n",
        "    Each record is a section headed '## ID: <n>' for easy Ctrl+F navigation.\n",
        "    The integer ID matches the 'id' column in the DataFrame.\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "    content = \"\\n\".join(format_eval_record_md(r, i) for i, r in enumerate(records))\n",
        "    path.write_text(content, encoding=\"utf-8\")\n",
        "    print(f\"Exported {len(records)} records → {path}\")\n",
        "\n",
        "\n",
        "export_eval_markdown(eval_records, OUTPUT_DIR / \"eval_questions.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98b6360",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "QUALITY_CUTOFF = 4\n",
        "PALETTE = {\"Before filter\": \"#4472C4\", \"After filter\": \"#ED7D31\"}\n",
        "\n",
        "\n",
        "def _build_before_after_df(\n",
        "    df_all: pd.DataFrame, df_filtered: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Combine all records and filtered records with a 'split' label column.\"\"\"\n",
        "    df_b = df_all[[\"doc_category\"] + SCORE_DIMS].copy()\n",
        "    df_b[\"split\"] = \"Before filter\"\n",
        "    df_a = df_filtered[[\"doc_category\"] + SCORE_DIMS].copy()\n",
        "    df_a[\"split\"] = \"After filter\"\n",
        "    return pd.concat([df_b, df_a], ignore_index=True)\n",
        "\n",
        "\n",
        "def _polish_bars(ax) -> None:\n",
        "    \"\"\"Add white bar edges and a subtle dashed horizontal grid.\"\"\"\n",
        "    for patch in ax.patches:\n",
        "        patch.set_edgecolor(\"white\")\n",
        "        patch.set_linewidth(0.8)\n",
        "    ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.6, alpha=0.6, color=\"#CCCCCC\")\n",
        "    ax.set_axisbelow(True)\n",
        "\n",
        "\n",
        "def plot_before_after_bar(\n",
        "    df_all: pd.DataFrame, df_filtered: pd.DataFrame, output_path: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Grouped bar chart: mean score per dimension, before vs after the quality filter.\n",
        "    Error bars show ±1 SD.\n",
        "    \"\"\"\n",
        "    combined = _build_before_after_df(df_all, df_filtered)\n",
        "    melted = combined.melt(\n",
        "        id_vars=[\"doc_category\", \"split\"], var_name=\"dimension\", value_name=\"score\"\n",
        "    )\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(9, 5))\n",
        "    sns.barplot(\n",
        "        data=melted, x=\"dimension\", y=\"score\", hue=\"split\",\n",
        "        palette=PALETTE, capsize=0.04, errorbar=\"sd\", ax=ax,\n",
        "        err_kws={\"linewidth\": 1.2},\n",
        "    )\n",
        "    _polish_bars(ax)\n",
        "    ax.axhline(y=QUALITY_CUTOFF, color=\"crimson\", linestyle=\"--\", linewidth=1.2,\n",
        "               label=f\"Quality cutoff ({QUALITY_CUTOFF})\", zorder=3)\n",
        "    ax.set_ylim(0, 5.5)\n",
        "    ax.set_title(\"Mean Score per Dimension — Before vs After Quality Filter\",\n",
        "                 fontsize=13, fontweight=\"bold\")\n",
        "    ax.set_xlabel(\"Dimension\")\n",
        "    ax.set_ylabel(\"Mean Score\")\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend(handles, labels, title=\"\", frameon=False)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved → {output_path}\")\n",
        "\n",
        "\n",
        "def plot_category_before_after_bar(\n",
        "    df_all: pd.DataFrame, df_filtered: pd.DataFrame, output_path: Path\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One subplot per critique dimension; each shows mean score by document category,\n",
        "    before vs after the quality filter. Error bars show ±1 SD.\n",
        "    \"\"\"\n",
        "    combined = _build_before_after_df(df_all, df_filtered)\n",
        "    melted = combined.melt(\n",
        "        id_vars=[\"doc_category\", \"split\"], var_name=\"dimension\", value_name=\"score\"\n",
        "    )\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(SCORE_DIMS), figsize=(15, 5), sharey=True)\n",
        "    fig.suptitle(\"Mean Score by Category — Before vs After Quality Filter\",\n",
        "                 fontsize=13, fontweight=\"bold\")\n",
        "\n",
        "    for ax, dim in zip(axes, SCORE_DIMS):\n",
        "        subset = melted[melted[\"dimension\"] == dim]\n",
        "        sns.barplot(\n",
        "            data=subset, x=\"doc_category\", y=\"score\", hue=\"split\",\n",
        "            palette=PALETTE, capsize=0.04, errorbar=\"sd\", ax=ax,\n",
        "            err_kws={\"linewidth\": 1.2},\n",
        "        )\n",
        "        _polish_bars(ax)\n",
        "        ax.axhline(y=QUALITY_CUTOFF, color=\"crimson\", linestyle=\"--\", linewidth=1.2, zorder=3)\n",
        "        ax.set_title(dim.capitalize(), fontsize=11)\n",
        "        ax.set_xlabel(\"Category\")\n",
        "        ax.set_ylabel(\"Mean Score\" if ax is axes[0] else \"\")\n",
        "        ax.set_ylim(0, 5.5)\n",
        "        ax.tick_params(axis=\"x\", rotation=20)\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    legend_elements = [\n",
        "        mpatches.Patch(facecolor=PALETTE[\"Before filter\"], edgecolor=\"white\", linewidth=0.8, label=\"Before filter\"),\n",
        "        mpatches.Patch(facecolor=PALETTE[\"After filter\"], edgecolor=\"white\", linewidth=0.8, label=\"After filter\"),\n",
        "    ]\n",
        "    fig.legend(handles=legend_elements, loc=\"upper right\", frameon=False)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved → {output_path}\")\n",
        "\n",
        "\n",
        "# Build the filtered DataFrame and run both plots\n",
        "df_filtered = build_eval_dataframe(filtered_records)\n",
        "\n",
        "plot_before_after_bar(df, df_filtered, OUTPUT_DIR / \"scores_before_after_bar.png\")\n",
        "plot_category_before_after_bar(df, df_filtered, OUTPUT_DIR / \"scores_by_category_before_after.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
